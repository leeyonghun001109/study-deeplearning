{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with keras - Ch2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WYqVZitH1M8U",
        "glhqe-EAn_64",
        "35mR2nK8EJqi"
      ],
      "authorship_tag": "ABX9TyPkl6hrHJobvWhtAEwTlEzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeyonghun001109/study-deeplearning/blob/main/Deep_Learning_with_keras_Ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzIPQeeFb6Fl"
      },
      "source": [
        "# **신경망의 수학적 구성 요소**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmuDDF21Ltr"
      },
      "source": [
        "# **2.1 신경망과의 첫 만남**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDpkS90tcAIy"
      },
      "source": [
        "흑백 손글씨 숫자 이미지 (28 x 28) 픽셀을 10개의 범주 (0~9)로 분류하고자 함\n",
        "\n",
        "MNIST 데이터 셋을 활용함\n",
        "\n",
        "> 6만 개의 train image  / 1만 개의 test image\n",
        "\n",
        "\n",
        "*NOTE 클래스와 레이블*\n",
        "\n",
        "분류 문제의 범주(category) : 클래스(class)\n",
        "\n",
        "데이터 포인트 : 샘플(sample)\n",
        "\n",
        "특정 샘플의 클래스 : 레이블 (label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M69YeJQPkRpz"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_images, train_labels : training set 구현\n",
        "\n",
        "test_iamges, test_labels : test set / 모델의 성능 test\n",
        "\n",
        "image : numpy 배열 / label : 0~9 숫자 배열\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0cM95lgj6h1",
        "outputId": "966fbe2c-45f1-43a3-c6ab-4d9f5a79f54c"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C25h8TmUkf2w",
        "outputId": "5a3e8242-d792-4fd3-b8e9-86e8947992ec"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(len(train_labels))\n",
        "print(train_labels)\n",
        "\n",
        "print(test_images.shape)\n",
        "print(len(test_labels))\n",
        "print(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "60000\n",
            "[5 0 4 ... 5 6 8]\n",
            "(10000, 28, 28)\n",
            "10000\n",
            "[7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wi3OEzHjmYO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "layer (층) : 신경망의 핵심 구성 요소 / 데이터 처리 필터 / 주어진 문제에 더 의미있는 representation(표현)을 입력된 데이터로 추출\n",
        "\n",
        "해당 코드의 경우 fully connected된 신경망 층인 Desnes 층 2개가 연속\n",
        "\n",
        "마지막 층의 경우 10개의 확률 점수가 들어 잇는 배열을 반환하는 Softmax 층\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYXamL_tkD1Z"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFfo2JJZunRI"
      },
      "source": [
        "\n",
        "\n",
        "1.   loss function (손실 함수) : 훈련 데이터에서 신경망의 성능을 측정하는 방법 / 네트워크가 옳은 방향으로 학습도리 수 있도록 도와줌\n",
        "\n",
        "2.   optimizer (옵티마이저) : 입력된 데이터와 loss function을 기반으로 네트워크를 업데이트하는 메커니즘\n",
        "\n",
        "3.  train과 test 과정을 모니터링할 지표 : accuracy(정확히 분류된 이미지의 비율)만 고려\n",
        "\n",
        "\n",
        "```\n",
        "network.compile(optimizer = 'rmsprop', loss = 'cateogrical_crossentropy', metrics = ['accuracy'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRqytIlJkH8d"
      },
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsSyGi-uinDd"
      },
      "source": [
        "훈련을 시작하기 전에 데이터를 네트워크에 맞는 크기로 바꾸고 모든 값을 0과 1 사이로 스케일 조정\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "train_images = train_images.reshape((60000, * 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((60000, * 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoqbNqDHkKkb"
      },
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKxqd8ByjYsM"
      },
      "source": [
        "레이블을 범주형으로 인코딩해야 함\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pzJv458kM9W"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-526k88jtU5"
      },
      "source": [
        "keras에서는 fit 메서드를 호출하여 훈련 데이터에 모델을 학습시킴\n",
        "\n",
        "```\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvOrTc0ujADO",
        "outputId": "7086f2dd-54e0-47db-d1e7-b18e1d40db08"
      },
      "source": [
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 15s 3ms/step - loss: 0.4300 - accuracy: 0.8748\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1136 - accuracy: 0.9657\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0720 - accuracy: 0.9792\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0475 - accuracy: 0.9861\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d202ef810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KdIJyDNqWl-"
      },
      "source": [
        "train하는 도중 2개의 정보, loss와 accuacry가 출력됨\n",
        "\n",
        "테스트의 정확도는 98.8%가 나옴 / 훈련 세트 정확도보다는 낮은데, overfiting이 발생했기 때문\n",
        "\n",
        "**overfiting**\n",
        "> 훈련 데이터보다 데이터에서 성능이 낮아지는 경향\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYqVZitH1M8U"
      },
      "source": [
        "# **2.2 신경망을 위한 데이터 표현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GRmNWO91SjS"
      },
      "source": [
        "tensor \n",
        "> 데이터를 위한 컨테이너와 유사 / 임의의 차원 개수를 가지는 행렬의 일반화된 모습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp8-Iz-d5ucr"
      },
      "source": [
        "**2.2.1 스칼라 (0D tensor)**\n",
        "\n",
        "scalar (스칼라)\n",
        "> 하나의 숫자만을 담고 있는 tensor\n",
        "\n",
        "> numpy : float32, float64 type의 숫자가 scalar tensor (scalar array)\n",
        "\n",
        "> ndim - numpy 배열의 축 개수를 확인할 수 있음\n",
        "\n",
        "> 랭크 (rank) : tensor의 축 개수 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a79ytIC6PJ7",
        "outputId": "ff6fd20b-2664-4b55-bf48-3744399de692"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehHJ9qih6eHu",
        "outputId": "8330cdce-84c3-4c60-87a0-3d99b36171ba"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scTzQVRR6f_c"
      },
      "source": [
        "**2.2.2 벡터 (1D tensor)**\n",
        "\n",
        "벡터 (vector)\n",
        "> 숫자열의 배열\n",
        "\n",
        "> 1D tensor : 딱 하나의 축을 가짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFMlFPB57DPr",
        "outputId": "05cbcf38-6c59-4fc6-f566-6551c92d7b01"
      },
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dmrjYl_7L4a",
        "outputId": "8fed8821-55e9-4992-fe7e-30598bc1ac56"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwOrEcaI7OSb"
      },
      "source": [
        "위의 벡터의 경우 5개의 원소를 지니고 있으므로 5차원 백터라고 부른다\n",
        "\n",
        "5d vector : 하나의 축을 따라 5개의 dimension을 지닌 것\n",
        "\n",
        "5d tensor : 5개의 축을 지닌 것\n",
        "\n",
        "차원수 (dimensionality) : 특정 축을 따라 놓인 원소의 개수, tensor의 축 개수를 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fZ2tueu7io8"
      },
      "source": [
        "**2.2.3 행렬 (2D tensor)**\n",
        "\n",
        "행렬 (matrix)\n",
        "> 벡터의 배열\n",
        "\n",
        "> 숫자가 채워진 사각 격자라고 생각하면 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhkpJLBL73Jd",
        "outputId": "13abe483-fcf6-4745-a8df-e756e5d49501"
      },
      "source": [
        "x = np.array([[1,2,3,4],\n",
        "             [5,6,7,8],\n",
        "             [9,10,11,12]])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4],\n",
              "       [ 5,  6,  7,  8],\n",
              "       [ 9, 10, 11, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-bYG1j88Fhb",
        "outputId": "54237fa5-d5ec-4a67-a138-7938577336c6"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tnj6lV78HvA"
      },
      "source": [
        "행 : 첫 번째 축에 놓여 있는 원소 ([1,2,3,4])\n",
        "\n",
        "렬 : 두 번째 축에 놓여 있는 원소 ([1,5,9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWySQzUd8Ste"
      },
      "source": [
        "**2.2.4 3D tensor와 고차원 tensor**\n",
        "\n",
        "ex) 3D tensor들을 하나의 배열로 합치면 4D tensor를 만드는 식으로 이어짐\n",
        "\n",
        "딥러닝에서는 보통 0D~4D까지의 tensor를 다룸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAyCU-K81mU"
      },
      "source": [
        "**2.2.5 핵심 속성**\n",
        "\n",
        "tensor : 3개의 속성으로 정의됨\n",
        "\n",
        "\n",
        "1.   축의 개수 (rank)\n",
        "    > 3D tensor에는 3개의 축이 있고, 행렬에는 2개의 축이 존재함 \n",
        "     \n",
        "    > numpy 라이브러리에서는 ndim 속성이 저장\n",
        "\n",
        "2.   크기 (shape)\n",
        "    > tensor의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 python의 tuple\n",
        "\n",
        "    > vector의 크기 : (5,) 처럼 1개의 원소로 이루어진 튜플\n",
        "\n",
        "    > 배열 scalar : () 처럼 크기가 없음\n",
        "\n",
        "3.   데이터 타입 (numpy - dtype에 저장)\n",
        "    > tensor에 포함된 데이터의 타입\n",
        "\n",
        "    > tensor - 사전에 할당되어 연속된 메머리에 저장되므로 numpy 배열은 가변 길이의 문자열을 지원하지 않음\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h_fVH0m-1fb"
      },
      "source": [
        "**2.2.6 numpy로 tensor 조작하기**\n",
        "\n",
        "slicing (슬라이싱) : 배열에 있는 특정 원소들을 선택하는 것\n",
        "\n",
        "각 배열의 축을 따라 어떤 인덱스 사이도 선택할 수 있음\n",
        "\n",
        "ex) 이미지 오른쪽 아래 14 x 14 픽셀을 선택 / 정중앙에 위치한 14 x 14 픽셀을 이미지에서 선택\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "my_slice = train_images[: , 14:, 14:]\n",
        "my_slice = train_images[: , 7-7:, 7:-7]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6-AIYgCANWV"
      },
      "source": [
        "**2.2.7 batch data**\n",
        "\n",
        "딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축 - sample axis (샘플축)\n",
        "\n",
        "딥러닝 모델의 경우 한 번에 전체 데이터셋을 처리하지 않고 작은 batch로 나누어 처리함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoTjrzz-BhOb"
      },
      "source": [
        "**2.2.8 tensor의 실제 사례**\n",
        "\n",
        "batch data \n",
        "> (samples, features) 크기의 2D tensor\n",
        "\n",
        "시계열 or sequence data\n",
        "> (samples, timesteps, features) 크기의 3D tensor\n",
        "\n",
        "image\n",
        "> (samples, height, weight, channels) or (samples, channels, height, width) 크기의 4D tensor\n",
        "\n",
        "video\n",
        "> (samples, frame, height, width, channels) or (samples, frames, channels, height, width) 크기의 5D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTDoxxBOf1ai"
      },
      "source": [
        "**2.2.9 vector data**\n",
        "\n",
        "첫 번째 축은 샘플 축, 두 번째 축은 특성 축 (feature axis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug0Q4vhZgCru"
      },
      "source": [
        "**2.2.10 시계열 data or sequence data**\n",
        "\n",
        "data - 시간 (연속된 순서)가 중요할 때는 시간 축을 포함하여 3D tensor로 저장함\n",
        "\n",
        "각 sample - vector의 seqeunce로 인코딩되므로 batch data는 3D tensor로 인코딩 될 것임\n",
        "\n",
        "{관례적으로 시간 축은 항상 두번째 축 (인덱스가 1인 축)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXO3LKQugcxi"
      },
      "source": [
        "**2.2.11 image data**\n",
        "\n",
        "image : height, width, channels 형식의 3D tensor\n",
        "\n",
        "image tensor를 저장하는 방식\n",
        "\n",
        "\n",
        "1.   channel - last  (채널 마지막) / 구글 텐서플로 - 컬러 채널의 깊이를 끝에 둠\n",
        "\n",
        "2.   channel - first (채널 우선) / 컬러 채널의 깊이를 배치 축 바로 뒤에 둠\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sff432CumcUE"
      },
      "source": [
        "**2.2.12 video data**\n",
        "\n",
        "video data : 5D tensor가 필요한 몇 안되는 데이터 중 하나임\n",
        "\n",
        "하나의 video - frame의 연속 / 각 frame은 하나의 color image\n",
        "\n",
        "frame : (height, width, color_depth) - 3D tensor로 저장\n",
        "\n",
        "frame의 연속 : (frames. height, width, color_depth) - 4D tensor로 저장\n",
        "\n",
        "여러 video : (samples, frames. height, width, color_depth) - 5D tensor로 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glhqe-EAn_64"
      },
      "source": [
        "# **2.3 신경망의 톱니바퀴  : tensor 연산**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-DHbmp7oFuy"
      },
      "source": [
        "심층 신경망이 학습한 모든 변환을 수치 data tensor에 적용하는 몇 종류의 tensor operation으로 나타낼 수 있음\n",
        "\n",
        "ex)\n",
        "\n",
        "\n",
        "```\n",
        "keras.layers.Dense(512, activation = 'relu')\n",
        "```\n",
        "해당 층을 2D tensor를 input으로 받고 input tensor의 새로운 표현인 또 다른 2D tensor를 반환하는 함수처럼 해석 가능함\n",
        "\n",
        "(W : 2D tensor / b : vector)\n",
        "\n",
        "```\n",
        "output = relu(dot(w, input) + b)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQemzfwEo28J"
      },
      "source": [
        "**2.3.1 원소별 연산**\n",
        "\n",
        "relu 함수와 덧셈 - (element-wise operation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1zpNsnP0TOb"
      },
      "source": [
        "**2.3.2 브로드캐스팅**\n",
        "\n",
        "단순한 덧셈 구현인 naive_add : 2D tensor + vector\n",
        "\n",
        "크기가 다른 두 tensor가 더해질 경우\n",
        "> (모호하지 않고 실행 가능하다면) 작은 tensor가 큰 tensor의 크기에 맞추어 broadcasting (브로드캐스팅)\n",
        "\n",
        "1.   큰 tensor의 ndim에 맞도록 작은 tensor에 (브로드캐스팅 축이라고 불리는) 축이 추가됨\n",
        "2.   작은 tensor가 새 축을 다라서 큰 tensor의 크기에 맞도록 반복됨\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnrg3zr8QIP"
      },
      "source": [
        "**2.3.3. tensor 점곱**\n",
        "\n",
        "tensor product (= dot operation) : 가장 널리 사용되고 유용한 텐서 연산임\n",
        "\n",
        "원소별 연산과 반대로 입력 텐서의 원소들을 결합\n",
        "\n",
        "넘파이, 케라스, 씨아노, 텐서플로 : 원소별 곱셈 -> * 연산자 사용함\n",
        "\n",
        "텐서플로 -> dot 연산자가 다름 / 넘파이와 케라스 -> 점곱 연산에 보편적인 dot 연산자를 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOYLQxa9ueR"
      },
      "source": [
        "**2.2.4 tensor 크기 변환**\n",
        "\n",
        "tensor의 크기 변환 - 특정 크기에 맞게 열과 행을 재배열한다는 뜻\n",
        "\n",
        "크기가 변환된 tensor는 원래 tensor와 원소 개수 동일함\n",
        "\n",
        "행렬의 전치(transpose) : 행과 열을 바꿔주는 것 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUb1xTARAFbe"
      },
      "source": [
        "**2.2.5 tensor 연산의 기하학적 해석**\n",
        "\n",
        "tensor 연산이 조작하는 tensor의 내용 - 기하학적 공간에 잇는 좌표 포인트로 해석 될 수 있기에 모든 tensor 연산은 기하학적 해석이 가능\n",
        "\n",
        "아핀 변환 (affine transformaiton), 회전, 스케일링 (scailing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy6XRTQJCO3u"
      },
      "source": [
        "**2.3.6 딥러닝의 기하학적 해석**\n",
        "\n",
        "신경망 - 전체적으로 tensor 연산의 연결로 구성된 것 / 모든 tensor 연산은 기하학적 변환임\n",
        "\n",
        "* 심층 네트워크의 각 층은 데이터를 조금씩 풀어주는 변환을 함\n",
        "\n",
        "* 해당 층을 깊게 쌓으면 아주 복잡한 분해 과정을 처리할 수 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpPO2MLKw8P"
      },
      "source": [
        "# **2.4 신경망의 엔진 : 그래디언트 기반 최적화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35mR2nK8EJqi"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "output = relu(dot(w, input) + b)\n",
        "```\n",
        "tensor w와 b는 층의 속성처럼 볼 수 있음\n",
        "\n",
        "가중치 (weight) or 훈련되는 피라미터 (trainable parameter)\n",
        "> 각각 커널 (kernel) , 편향 (bias)라고 부름\n",
        "\n",
        "> weight에는 train data를 신경망에 노출시켜서 학습된 정보가 담겨있음\n",
        "\n",
        "> 가중치 행렬이 작은 난수로 채워져 있음 (무작위 초기화 (random initialization) 단계)\n",
        "\n",
        "> W와 b가 난수일 때 relu(dot(w, input) + b)이 어떤 표현을 만들 것인지는 예측 불가능함\n",
        "\n",
        "> 이후 피드백 신호에 기초하여 weight가 점진적으로 조정됨\n",
        "\n",
        " \n",
        "\n",
        "train은 training loop (훈련 반복 루프)내부에서 일어나게 됨\n",
        "\n",
        "1. 훈련 샘플 x와 이에 상응하는 타깃 y의 배치를 추출함\n",
        "\n",
        "2. x를 사용하여 네트워크를 실행하고 (정방향 패스 (forward pass)단계, 예측 y_pred를 구함)\n",
        "\n",
        "3. y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산\n",
        "\n",
        "4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 weight를 업데이트\n",
        "\n",
        "따라서 train data에서 네트워크의 loss, 즉 예측 y_pred와 타깃 y의 오차가 매우 작아질 것임\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk7V5wz6GG2Y"
      },
      "source": [
        "개별적인 weight를 업데이트 하는 방법\n",
        "\n",
        "> 네트워크 weight 행렬의 원소를 고정하고 관심있는 하나만 다른 값을 적용\n",
        "\n",
        "ex)\n",
        "> weight의 초깃값을 0.3으로 설정 -> batch data를 정방향 패스에 통과시킨 후 네트워크 손실 0.5로 나옴\n",
        "\n",
        "> weight 0.35로 변경 -> 정방향 패스 실행 -> 손실이 0.6으로 증가함\n",
        "\n",
        "> weight 0.25로 감소 -> 손실이 0.4로 감소\n",
        "\n",
        "> weight를 -0.05만큼 업데이트한 것이 손실을 줄이는데 기여한 것임\n",
        "\n",
        "해당 접근 방식의 경우 weight 행렬의 원소마다 두번의 정방향 패스를 계산해야하므로 비효율적임\n",
        "\n",
        "신경망에 사용된 모든 연산이  differentiable(미분 가능)하다는 장점을 사용하여 네트워크 weight에 대한 손실의 gradient를 계산하는 것이 더 좋음\n",
        "\n",
        "gradient의 반대 방향으로 weight를 이동하면 손실이 감소됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVBni5TQ6OAR"
      },
      "source": [
        "**2.4.1 변화율?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxdpNU1k6XRS"
      },
      "source": [
        "**2.4.2 tensor 연산의 변화율 : gradient**\n",
        "\n",
        "gradient : tensor operation의 변화율 / 다차원 입력, 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것\n",
        "\n",
        "입력 vector : x , 행렬 : w, 타깃 : y,  loss function : loss가 있다고 가정\n",
        "> W를 사용하여 타깃의 예측 y_pred를 계산하고 손실, 즉 타깃의 예측 y_pred와 타깃 y사이의 오차를 계산할 수 있음\n",
        "\n",
        "\n",
        "함수 f(x) : 변화율의 반대 방향으로 x를 조금 움직이면 f(x)의 값을 감소시킬 수 있음\n",
        "\n",
        "동일한 방식으로 함수 f(W)의 입장에서는 gradient의 반대 방향으로 W를 움직이면 f(W)의 값을 줄일 수 있음\n",
        "\n",
        "ex) \n",
        "> W1 = W0 - step * gradient(f)(W0)\n",
        "\n",
        "> gradient(f)(W0) : W0에 아주 근접해 있을 때 기울기를 근사한 것으로 W0에서 크게 벗어나지 않기 위해 스케일링 비율 step이 필요함\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj5fsBEhEvkn"
      },
      "source": [
        "**2.4.3 확률적 경사 하강법**\n",
        "\n",
        "미분 가능한 함수가 주어지면 이론적으로 해당 함수의 최솟값을 해석적으로 구할 수 있음\n",
        "\n",
        "함수의 최솟값 - 변화율이 0인 지점\n",
        "\n",
        "신경망에 적용할 경우 가장 loss function의 값을 만드는 weight의 조합을 해석적으로 찾는 것을 의미\n",
        "\n",
        "\n",
        "\n",
        "1.   미니 배치 확률적 경사 하강법 (mini- batch stochastic gradient descent / 미니배치 SGD)\n",
        "\n",
        "\n",
        "  *   확률적 (stochastic) :  각 배치 데이터가 무작위로 선택된다는 의미  \n",
        "  *   step 값을 적절히 고르는 것이 중요 ( 값이 너무 작으면 곡선을 따라 내려가는데 너무 많은 반복이 필요하며, local minimum에 갇힐 수 있음 / 값이 클 경우 loss function 곡선에서 임의의 위치로 이동시킬 수 있음 )\n",
        "\n",
        "\n",
        "2.   배치 SGD ( batch SGD )\n",
        "\n",
        "  *  가용한 모든 데이터를 사용하여 반복을 실행\n",
        "\n",
        "3.  SGD 변종\n",
        "\n",
        "  *  현재 gradient 값만 보지 않고 이전에 업데이트된 weight를 여러 가지 다른 방식으로 고려\n",
        "  *   모멘텀을 사용한 SGD, RMSProp, Adagrad (Momntum : SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결함)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CezZPL5Itgn"
      },
      "source": [
        "**2.4.4 변화율 연결 : 역전파 알고리즘**\n",
        "\n",
        "역전파 - 최종 손실 값에서부터 시작됨\n",
        "\n",
        "손실 값에 각 파라미터가 기여한 정도를 계산하기 위해 연쇄 법칙을 적용하여 최상위 층에서 하위층까지 거꾸로 진행됨\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC02mtiWK6NR"
      },
      "source": [
        "# **2.5 첫 예제 다시 살펴보기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpwF4RpzK-BH"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "```\n",
        "\n",
        "input_image의 data type  : float 32 \n",
        "\n",
        "train_data : (60000, 784) size , test_data : (10000, 784) size numpy 배열로 저장\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "-> 사용할 신경망\n",
        "\n",
        "2개의 Dense층이 존재하며, 각 층은 weight tensor를 포함하여 input data에 대한 몇 개의 간단한 tensor 연산을 적용함\n",
        "\n",
        "층의 속성인 weight tensor는 네트워크가 정보를 저장하는 곳\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "```\n",
        "-> 네트워크를 compile하는 단계\n",
        "\n",
        "categorical_crossentropy : loss function (weight tensor를 학습하기 위한 피드백 신호로 사용, 훈련하는 동안 최소화됨)\n",
        "\n",
        "미니 배치 확률적 경사 하강법을 통해 손실이 감소됨\n",
        "\n",
        "경사 하강법을 적용하는 구체적인 방식은 첫 번째 매개변수로 전달된 rmsprop optimizer에 의해 결정됨\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "```\n",
        "\n",
        "-> 훈련 반복\n",
        "\n",
        "네트워크가 128개 샘플씩 mini-batch로 train data를 다섯 번 반복함 (전체 train data에 수행되는 각 반복 : epoch)\n",
        "\n",
        "각 반복마다 네트워크가 batch에서 손실에 대한 weight의 gradient를 계산하고 그에 맞추어 weight를 업데이트함\n",
        "\n",
        "다섯 번의 epoch동안 네트워크는 2,345번의 gradient 업데이트를 수행함 (1 epoch당 469번)\n",
        "\n",
        "네트워크의 손실이 충분하게 낮아져서 높은 정확도로 구분 가능함\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8BoeVRkQ9Xi"
      },
      "source": [
        "# **2.6 요약**\n",
        "\n",
        "\n",
        "\n",
        "1.    learning : train data sample과 그에 상응하는 타깃이 주어졌을 때 loss function을 최소화 하는 model parameter의 조합을 찾는 것\n",
        "\n",
        "2.    data sample과 타깃의 batch를 랜덤하게 봅고 이 batch에서 loss에 대한 parameter gradient를 계산함으로써 trainning됨 \n",
        "\n",
        "     network의 parameter는 gradient의 반대 방향으로 조금씩 움직임\n",
        "\n",
        "3.    전체 trainning 과정은 신경망이 미분 가능한 tensor operation으로 연결되어있기에 가능함 \n",
        "\n",
        "     현재 parameter와 batch data를 graident 값에 매핑해주는 gradient 함수를 구성하기 위해 미분의 연쇄 법칙을 사용함\n",
        "\n",
        "\n",
        "4. loss와 optimizer의 경우 network에 data를 주입하기 전에 define\n",
        "\n",
        "5.  loss : trainning에 있어서 최소화해야 할 양이므로 해결하려는 문제의 성공을 측정하는데 사용\n",
        "\n",
        "6.  optimizer : loss 에 대한 gradient가 parameter를 업데이트하는 정확한 방식을 정의 / ex ) RMSProp optimizer, SGD (using momentum)\n",
        "\n",
        "\n"
      ]
    }
  ]
}